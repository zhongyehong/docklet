import time, threading, random, string, os, traceback
import master.monitor
import subprocess,json
from functools import wraps

from utils.log import initlogging, logger
from utils import env

class BatchJob(object):
    def __init__(self, user, job_info):
        self.user = user
        self.raw_job_info = job_info
        self.job_id = None
        self.job_name = job_info['jobName']
        self.job_priority = int(job_info['jobPriority'])
        self.status = 'pending'
        self.create_time = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime())
        self.lock = threading.Lock()
        self.tasks = {}
        self.dependency_out = {}
        self.tasks_cnt = {'pending':0, 'scheduling':0, 'running':0, 'retrying':0, 'failed':0, 'finished':0}

        #init self.tasks & self.dependency_out & self.tasks_cnt
        logger.debug("Init BatchJob user:%s job_name:%s create_time:%s" % (self.user, self.job_name, self.create_time))
        raw_tasks = self.raw_job_info["tasks"]
        self.tasks_cnt['pending'] = len(raw_tasks.keys())
        for task_idx in raw_tasks.keys():
            task_info = raw_tasks[task_idx]
            self.tasks[task_idx] = {}
            self.tasks[task_idx]['config'] = task_info
            self.tasks[task_idx]['status'] = 'pending'
            self.tasks[task_idx]['dependency'] = []
            dependency = task_info['dependency'].strip().replace(' ', '').split(',')
            if len(dependency) == 1 and dependency[0] == '':
                continue
            for d in dependency:
                if not d in raw_tasks.keys():
                    raise ValueError('task %s is not defined in the dependency of task %s' % (d, task_idx))
                self.tasks[task_idx]['dependency'].append(d)
                if not d in self.dependency_out.keys():
                    self.dependency_out[d] = []
                self.dependency_out[d].append(task_idx)

        self.log_status()
        logger.debug("BatchJob(id:%s) dependency_out: %s" % (self.job_id, json.dumps(self.dependency_out, indent=3)))

    def data_lock(f):
        @wraps(f)
        def new_f(self, *args, **kwargs):
            self.lock.acquire()
            try:
                result = f(self, *args, **kwargs)
            except Exception as err:
                self.lock.release()
                raise err
            self.lock.release()
            return result
        return new_f

    # return the tasks without dependencies
    @data_lock
    def get_tasks_no_dependency(self,update_status=False):
        logger.debug("Get tasks without dependencies of BatchJob(id:%s)" % self.job_id)
        ret_tasks = []
        for task_idx in self.tasks.keys():
            if (self.tasks[task_idx]['status'] == 'pending' and
                len(self.tasks[task_idx]['dependency']) == 0):
                if update_status:
                    self.tasks_cnt['pending'] -= 1
                    self.tasks_cnt['scheduling'] += 1
                    self.tasks[task_idx]['status'] = 'scheduling'
                task_name = self.job_id + '_' + task_idx
                ret_tasks.append([task_name, self.tasks[task_idx]['config'], self.job_priority])
        self.log_status()
        return ret_tasks

    # update status of this job based
    def _update_job_status(self):
        allcnt = len(self.tasks.keys())
        if self.tasks_cnt['failed'] != 0:
            self.status = 'failed'
        elif self.tasks_cnt['running'] != 0:
            self.status = 'running'
        elif self.tasks_cnt['finished'] == allcnt:
            self.status = 'done'
        else:
            self.status = 'pending'

    # start run a task, update status
    @data_lock
    def update_task_running(self, task_idx):
        logger.debug("Update status of task(idx:%s) of BatchJob(id:%s) running." % (task_idx, self.job_id))
        old_status = self.tasks[task_idx]['status'].split('(')[0]
        self.tasks_cnt[old_status] -= 1
        self.tasks[task_idx]['status'] = 'running'
        self.tasks_cnt['running'] += 1
        self._update_job_status()
        self.log_status()

    # a task has finished, update dependency and return tasks without dependencies
    @data_lock
    def finish_task(self, task_idx):
        if task_idx not in self.tasks.keys():
            logger.error('Task_idx %s not in job. user:%s job_name:%s job_id:%s'%(task_idx, self.user, self.job_name, self.job_id))
            return []
        logger.debug("Task(idx:%s) of BatchJob(id:%s) has finished. Update dependency..." % (task_idx, self.job_id))
        old_status = self.tasks[task_idx]['status'].split('(')[0]
        self.tasks_cnt[old_status] -= 1
        self.tasks[task_idx]['status'] = 'finished'
        self.tasks_cnt['finished'] += 1
        self._update_job_status()
        if task_idx not in self.dependency_out.keys():
            self.log_status()
            return []
        ret_tasks = []
        for out_idx in self.dependency_out[task_idx]:
            try:
                self.tasks[out_idx]['dependency'].remove(task_idx)
            except Exception as err:
                logger.warning(traceback.format_exc())
                continue
            if (self.tasks[out_idx]['status'] == 'pending' and
                len(self.tasks[out_idx]['dependency']) == 0):
                self.tasks_cnt['pending'] -= 1
                self.tasks_cnt['scheduling'] += 1
                self.tasks[out_idx]['status'] = 'scheduling'
                task_name = self.job_id + '_' + out_idx
                ret_tasks.append([task_name, self.tasks[out_idx]['config'], self.job_priority])
        self.log_status()
        return ret_tasks

    # update retrying status of task
    @data_lock
    def update_task_retrying(self, task_idx, reason, tried_times):
        logger.debug("Update status of task(idx:%s) of BatchJob(id:%s) retrying. reason:%s tried_times:%d" % (task_idx, self.job_id, reason, int(tried_times)))
        old_status = self.tasks[task_idx]['status'].split('(')[0]
        self.tasks_cnt[old_status] -= 1
        self.tasks_cnt['retrying'] += 1
        self.tasks[task_idx]['status'] = 'retrying(%s)(%d times)' % (reason, int(tried_times))
        self._update_job_status()
        self.log_status()

    # update failed status of task
    @data_lock
    def update_task_failed(self, task_idx, reason, tried_times):
        logger.debug("Update status of task(idx:%s) of BatchJob(id:%s) failed. reason:%s tried_times:%d" % (task_idx, self.job_id, reason, int(tried_times)))
        old_status = self.tasks[task_idx]['status'].split('(')[0]
        self.tasks_cnt[old_status] -= 1
        self.tasks_cnt['failed'] += 1
        if reason == "OUTPUTERROR":
            self.tasks[task_idx]['status'] = 'failed(OUTPUTERROR)'
        else:
            self.tasks[task_idx]['status'] = 'failed(%s)(%d times)' % (reason, int(tried_times))
        self._update_job_status()
        self.log_status()

    # print status for debuging
    def log_status(self):
        task_copy = {}
        for task_idx in self.tasks.keys():
            task_copy[task_idx] = {}
            task_copy[task_idx]['status'] = self.tasks[task_idx]['status']
            task_copy[task_idx]['dependency'] = self.tasks[task_idx]['dependency']
        logger.debug("BatchJob(id:%s) tasks status: %s" % (self.job_id, json.dumps(task_copy, indent=3)))
        logger.debug("BatchJob(id:%s)  tasks_cnt: %s" % (self.job_id, self.tasks_cnt))
        logger.debug("BatchJob(id:%s)  job_status: %s" %(self.job_id, self.status))


class JobMgr():
    # load job information from etcd
    # initial a job queue and job schedueler
    def __init__(self, taskmgr):
        self.job_queue = []
        self.job_map = {}
        self.taskmgr = taskmgr
        self.fspath = env.getenv('FS_PREFIX')

    # user: username
    # job_info: a json string
    # user submit a new job, add this job to queue and database
    def add_job(self, user, job_info):
        try:
            job = BatchJob(user, job_info)
            job.job_id = self.gen_jobid()
            self.job_map[job.job_id] = job
            self.process_job(job)
        except ValueError as err:
            logger.error(err)
            return [False, err.args[0]]
        except Exception as err:
            logger.error(traceback.format_exc())
            #logger.error(err)
            return [False, err.args[0]]
        return [True, "add batch job success"]

    # user: username
    # list a user's all job
    def list_jobs(self,user):
        res = []
        for job_id in self.job_map.keys():
            job = self.job_map[job_id]
            logger.debug('job_id: %s, user: %s' % (job_id, job.user))
            if job.user == user:
                all_tasks = job.raw_job_info['tasks']
                tasks_vnodeCount = {}
                for task in all_tasks.keys():
                    tasks_vnodeCount[task] = int(all_tasks[task]['vnodeCount'])
                res.append({
                    'job_name': job.job_name,
                    'job_id': job.job_id,
                    'status': job.status,
                    'create_time': job.create_time,
                    'tasks': list(all_tasks.keys()),
                    'tasks_vnodeCount': tasks_vnodeCount
                })
        res.sort(key=lambda x:x['create_time'],reverse=True)
        return res

    # user: username
    # jobid: the id of job
    # get the information of a job, including the status, json description and other information
    # call get_task to get the task information
    def get_job(self, user, job_id):
        pass

    # check if a job exists
    def is_job_exist(self, job_id):
        return job_id in self.job_map.keys()

    # generate a random job id
    def gen_jobid(self):
        job_id = ''.join(random.sample(string.ascii_letters + string.digits, 8))
        while self.is_job_exist(job_id):
            job_id = ''.join(random.sample(string.ascii_letters + string.digits, 8))
        return job_id

    # add tasks into taskmgr's queue
    def add_task_taskmgr(self, user, tasks):
        for task_name, task_info, task_priority in tasks:
            if not task_info:
                logger.error("task_info does not exist! task_name(%s)" % task_name)
                return False
            else:
                logger.debug("Add task(name:%s) with priority(%s) to taskmgr's queue." % (task_name, task_priority) )
                self.taskmgr.add_task(user, task_name, task_info, task_priority)
        return True

    # to process a job, add tasks without dependencies of the job into taskmgr
    def process_job(self, job):
        tasks = job.get_tasks_no_dependency(True)
        return self.add_task_taskmgr(job.user, tasks)

    # report task status from taskmgr when running, failed and finished
    # task_name: job_id + '_' + task_idx
    # status: 'running', 'finished', 'retrying', 'failed'
    # reason: reason for failure or retrying, such as "FAILED", "TIMEOUT", "OUTPUTERROR"
    # tried_times: how many times the task has been tried.
    def report(self, user, task_name, status, reason="", tried_times=1):
        split_task_name = task_name.split('_')
        if len(split_task_name) != 2:
            logger.error("Illegal task_name(%s) report from taskmgr" % task_name)
            return
        job_id, task_idx = split_task_name
        job  = self.job_map[job_id]
        if status == "running":
            job.update_task_running(task_idx)
        elif status == "finished":
            next_tasks = job.finish_task(task_idx)
            if len(next_tasks) == 0:
                return
            ret = self.add_task_taskmgr(user, next_tasks)
        elif status == "retrying":
            job.update_task_retrying(task_idx, reason, tried_times)
        elif status == "failed":
            job.update_task_failed(task_idx, reason, tried_times)

    # Get Batch job stdout or stderr from its file
    def get_output(self, username, jobid, taskid, vnodeid, issue):
        filename = jobid + "_" + taskid + "_" + vnodeid + "_" + issue + ".txt"
        fpath = "%s/global/users/%s/data/batch_%s/%s" % (self.fspath,username,jobid,filename)
        logger.info("Get output from:%s" % fpath)
        try:
            ret = subprocess.run('tail -n 100 ' + fpath,stdout=subprocess.PIPE,stderr=subprocess.STDOUT, shell=True)
            if ret.returncode != 0:
                raise IOError(ret.stdout.decode(encoding="utf-8"))
        except Exception as err:
            logger.error(traceback.format_exc())
            return ""
        else:
            return ret.stdout.decode(encoding="utf-8")
